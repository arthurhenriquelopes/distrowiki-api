# ğŸ¤– GitHub Actions Scraper

Sistema de scraping automatizado usando GitHub Actions que roda diariamente nos servidores do GitHub.

## ğŸ“‹ Como Funciona

1. **GitHub Actions** roda automaticamente todos os dias Ã s 3h UTC
2. Instala Python, Playwright e Chromium
3. Executa o scraper do DistroWatch
4. Salva resultados em `data/cache/distros_scraped.json`
5. Faz commit automÃ¡tico dos dados atualizados
6. API lÃª os dados do arquivo JSON

## ğŸš€ Vantagens

- âœ… **100% Gratuito** - usa infraestrutura do GitHub
- âœ… **Sem bloqueios de rede** - servidores do GitHub tÃªm acesso ao DistroWatch
- âœ… **AutomÃ¡tico** - roda diariamente sem intervenÃ§Ã£o
- âœ… **HistÃ³rico** - commits mostram histÃ³rico de atualizaÃ§Ãµes
- âœ… **Pode executar manualmente** via interface do GitHub

## ğŸ“¦ Arquivos

```
.github/workflows/
  â””â”€â”€ scrape-distrowatch.yml    # GitHub Action workflow
scrape_runner.py                 # Script Python standalone
data/cache/
  â””â”€â”€ distros_scraped.json       # Dados scraped (auto-gerado)
```

## ğŸ¯ Como Usar

### 1. Ativar GitHub Actions

1. VÃ¡ em `Settings` > `Actions` > `General`
2. Ative "Allow all actions and reusable workflows"
3. Em "Workflow permissions", marque "Read and write permissions"

### 2. Executar Manualmente

1. VÃ¡ em `Actions` no GitHub
2. Clique em "Scrape DistroWatch"
3. Clique em "Run workflow"
4. Selecione branch `main`
5. Clique em "Run workflow"

### 3. Acessar Dados via API

```bash
# Dados do Ãºltimo scraping
curl https://api.distrowiki.com/scraping/scraped-data

# Status do scraping local
curl https://api.distrowiki.com/scraping/status
```

## â° Schedule

Por padrÃ£o, roda **diariamente Ã s 3h UTC (0h BRT)**. Para mudar:

```yaml
schedule:
  - cron: '0 3 * * *'  # 3h UTC todos os dias
  # - cron: '0 */6 * * *'  # A cada 6 horas
  # - cron: '0 0 * * 0'    # Aos domingos Ã  meia-noite
```

## ğŸ” Monitoramento

- **Logs**: `Actions` > `Scrape DistroWatch` > Ver run especÃ­fico
- **Commits**: HistÃ³rico mostra atualizaÃ§Ãµes automÃ¡ticas
- **Arquivo**: `data/cache/distros_scraped.json` tem timestamp

## ğŸ“Š Estrutura dos Dados

```json
{
  "scraped_at": "2025-11-19T22:00:00",
  "scraped_by": "github-actions",
  "total": 100,
  "distros": [
    {
      "rank": "1",
      "name": "MX Linux",
      "url": "https://distrowatch.com/table.php?distribution=mx",
      "popularity_score": "2847"
    }
  ],
  "metadata": {
    "source": "distrowatch.com",
    "scraper": "playwright",
    "version": "1.0.0"
  }
}
```

## ğŸ› ï¸ Desenvolvimento Local

Testar o scraper localmente:

```bash
# Instalar dependÃªncias
pip install playwright beautifulsoup4 lxml playwright-stealth
playwright install chromium

# Executar scraper
python scrape_runner.py
```

## ğŸš¨ Troubleshooting

### Action falha com erro de permissÃ£o
- Verifique se "Read and write permissions" estÃ¡ ativado
- Settings > Actions > General > Workflow permissions

### Scraping retorna 0 distros
- Verifique logs do Action para ver erros especÃ­ficos
- DistroWatch pode estar temporariamente indisponÃ­vel

### Dados nÃ£o aparecem na API
- Verifique se o arquivo `data/cache/distros_scraped.json` existe
- Certifique-se que o Action completou com sucesso

## ğŸ“ˆ Limites do GitHub Actions

- âœ… 2.000 minutos/mÃªs (Free tier)
- âœ… Cada run ~5-10 minutos
- âœ… ~200-400 execuÃ§Ãµes/mÃªs possÃ­veis
- âœ… Suficiente para scraping diÃ¡rio

## ğŸ” SeguranÃ§a

- NÃ£o requer tokens ou credenciais extras
- Usa `GITHUB_TOKEN` automÃ¡tico do Actions
- Commits assinados pelo bot do GitHub
- Dados pÃºblicos do DistroWatch

## ğŸ“ Notas

- **Primeira execuÃ§Ã£o**: FaÃ§a manualmente para testar
- **Timezone**: UTC (ajuste cron se necessÃ¡rio)
- **Limit**: PadrÃ£o Ã© 100 distros (top 100)
- **Fallback**: API local ainda funciona independentemente
